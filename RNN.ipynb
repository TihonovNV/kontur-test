{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "После простых методов решения я решил попробовать готовое решение для задачи truecasing-а.\n",
    "\n",
    "В данном ноутбуке я подготовил train.txt для обучения character-level RNN (а точнее LSTM). <br/>\n",
    "[Github](https://github.com/raymondhs/pytorch-char-rnn-truecase)<br/>\n",
    "[Original paper](https://www.aclweb.org/anthology/D16-1225/)<br/>\n",
    "Изначально я пытался обучить на полных названиях компаний из train.txt, но на 11 мегайбайтах от изначального датасета моя нейронка обучалась 2 часа. Также, когда я применил обученную нейронку на test.txt, мне показало, что 56 мегабайт будут преобразовываться 8 часов, что не особо круто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_companies = open('../kontur_srs_internship_test_task/train.txt', 'r').readlines()\n",
    "train_companies = list(map(lambda s: s.strip(), train_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_companies[471970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(company, spaces=False):\n",
    "    with_spaces = list(filter(None, re.split('(\\w+| )', company)))\n",
    "    if(spaces):\n",
    "        return with_spaces\n",
    "    return [token for token in with_spaces if token != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'w') as input_file:\n",
    "    for company in train_companies:\n",
    "        input_file.write(company + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Слова со смешанным регистром\n",
    "Следом мне пришла другая идея - попробовать обучить нейросеть на словах со смешанным регистром, так как с другими типами слов всё намного проще, однако у данной нейросети не получилось выявить закономерность, лосс не уменьшался и большинство преобразованных слов были просто с заглавной буквы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск слов со смешанным регистром для трейн сета\n",
    "\n",
    "mixed_words_train = []\n",
    "for company in train_companies[:-1000000]:\n",
    "    for word in tokenize(company):\n",
    "        if not word[1:].islower() and not word[1:].isupper() and len(word) > 1:\n",
    "            mixed_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set for RNN\n",
    "\n",
    "with open('input.txt', 'w') as input_file:\n",
    "    for mixed_word in mixed_words_train:\n",
    "        input_file.write(mixed_word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск слов со смешанным регистром для тест сета\n",
    "\n",
    "mixed_words_test = []\n",
    "for company in train_companies[-1000000:]:\n",
    "    for word in tokenize(company):\n",
    "        if not word[1:].islower() and not word[1:].isupper() and len(word) > 1:\n",
    "            mixed_words_test.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set for RNN\n",
    "\n",
    "test_lower = open('test.lower.txt', 'w')\n",
    "test = open('test.txt', 'w')\n",
    "for mixed_word in mixed_words_test:\n",
    "    test_lower.write(mixed_word.lower() + '\\n')\n",
    "    test.write(mixed_word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование\n",
    "(Взято из оригинальной документации)\n",
    "\n",
    "### Training\n",
    "\n",
    "```\n",
    "python train.py \\\n",
    "  -data_dir data/wiki \\\n",
    "  -rnn_size 700 \\\n",
    "  -num_layers 3 \\\n",
    "  -dropout 0.25 \\\n",
    "  -batch_size 100 \\\n",
    "  -seq_length 50 \\\n",
    "  -max_epochs 30 \\\n",
    "  -learning_rate 0.001 \\\n",
    "  -checkpoint_dir cv/wiki_lstm_700hidden_3layer \\\n",
    "  -gpuid 0\n",
    "```\n",
    "\n",
    "### Truecasing\n",
    "\n",
    "```\n",
    "# retrieve best checkpoint on valid data\n",
    "model=`ls cv/wiki_lstm_700hidden_3layer/*.pt | python best_model.py`\n",
    "\n",
    "cat data/wiki/test.lower.txt \\\n",
    "| python truecase.py \\\n",
    "    $model \\\n",
    "    -beamsize 10 \\\n",
    "    -verbose 0 \\\n",
    "    -gpuid 0 \\\n",
    "> data/wiki/output.txt\n",
    "\n",
    "# calculate performance on test set\n",
    "python word_eval.py data/wiki/test.txt cv/wiki_lstm_700hidden_3layer/output.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
