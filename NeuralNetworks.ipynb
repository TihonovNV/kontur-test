{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "<font size='4'>В этом ноутбуке я попытался применить нейросети с полносвязными слоями для решения задачи. К сожалению, к успеху это не привело</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Длина максимального слова (для нормализации), подробнее в 'Testing.ipynb'\n",
    "MAX_WORD = 57\n",
    "\n",
    "CONSONANTS = 'бвгджзйклмнпрстфхцчшщБВГДЖЗЙКЛМНПРСТФХЦЧШЩbcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXZ'\n",
    "VOWELS = 'аиоуэыеёюяАИОУЭЫЕЁЮЯaeiouyAEIOUY'\n",
    "ALPHABET = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяaeiouyAEIOUYbcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_companies = open('../kontur_srs_internship_test_task/train.txt', 'r').readlines()\n",
    "train_companies = list(map(lambda s: s.strip(), train_companies))\n",
    "\n",
    "test_companies = open('../kontur_srs_internship_test_task/test.txt', 'r').readlines()\n",
    "test_companies = list(map(lambda s: s.strip(), test_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_companies[471970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "               \n",
    "def tokenize(company, spaces=False):\n",
    "    with_spaces = list(filter(None, re.split('(\\w+| )', company)))\n",
    "    if(spaces):\n",
    "        return with_spaces\n",
    "    return [token for token in with_spaces if token != ' ']\n",
    "\n",
    "def has_letters(string):\n",
    "    return any(char in ALPHABET for char in string)\n",
    "\n",
    "def has_not_letters(string):\n",
    "    return any(char not in ALPHABET for char in string)\n",
    "\n",
    "def count_vowels(string):\n",
    "    return len([char for char in string if char in VOWELS])\n",
    "\n",
    "def count_consonants(string):\n",
    "    return len([char for char in string if char in CONSONANTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(company, test=False):\n",
    "    \n",
    "    # Длина слова, количество гласных/согласных, положение в слове,\n",
    "    # есть ли в слове что-то помимо букв, какой символ окружает слово\n",
    "    # и то же самое для предыдущего и последующего слова\n",
    "    \n",
    "    num_of_features = 15 + MAX_WORD\n",
    "    length = num_of_features * 3\n",
    "    \n",
    "    # 3 класса - полностью писменными, первая заглавная или полностью заглавными\n",
    "    num_of_classes = 3\n",
    "\n",
    "    size = 0\n",
    "    for word in tokenize(company):\n",
    "        if(has_letters(word)):\n",
    "            size += 1\n",
    "            \n",
    "    features = np.zeros((size, length))\n",
    "    labels = np.zeros(size)\n",
    "    \n",
    "    index = 0\n",
    "    tokens = tokenize(company, spaces=True)\n",
    "    for i, word in enumerate(tokens):\n",
    "        if(has_letters(word)):\n",
    "            \n",
    "            # Генерация фич\n",
    "            \n",
    "            if(index == 0):\n",
    "                features[index][num_of_features] = index/size\n",
    "                features[index][num_of_features + 1] = len(word)/MAX_WORD\n",
    "                features[index][num_of_features + 2] = count_consonants(word)/len(word)\n",
    "                features[index][num_of_features + 3] = count_vowels(word)/len(word)\n",
    "                features[index][num_of_features + 4] = int(has_not_letters(word))\n",
    "                if(i > 0):\n",
    "                    features[index][num_of_features + 5] = int(tokens[i - 1] == '\"')\n",
    "                    features[index][num_of_features + 6] = int(tokens[i - 1] == '(')\n",
    "                    features[index][num_of_features + 7] = int(tokens[i - 1] == ' ')\n",
    "                    features[index][num_of_features + 8] = int(tokens[i - 1] == ')')\n",
    "                    features[index][num_of_features + 9] = int(tokens[i - 1] == '-')\n",
    "                if( i + 1 < len(tokens)):\n",
    "                    features[index][num_of_features + 10] = int(tokens[i + 1] == ')')\n",
    "                    features[index][num_of_features + 11] = int(tokens[i + 1] == '\"')\n",
    "                    features[index][num_of_features + 12] = int(tokens[i + 1] == ' ')\n",
    "                    features[index][num_of_features + 13] = int(tokens[i + 1] == ')')\n",
    "                    features[index][num_of_features + 14] = int(tokens[i + 1] == '-')\n",
    "                    \n",
    "                for j, char in enumerate(list(word)):\n",
    "                    features[index][num_of_features + 15 + j] = encoding[char.upper()]\n",
    "                \n",
    "            else:\n",
    "                features[index - 1][2 * num_of_features] = index/size\n",
    "                features[index - 1][2 * num_of_features + 1] = len(word)/MAX_WORD\n",
    "                features[index - 1][2 * num_of_features + 2] = count_consonants(word)/len(word)\n",
    "                features[index - 1][2 * num_of_features + 3] = count_vowels(word)/len(word)\n",
    "                features[index - 1][2 * num_of_features + 4] = int(has_not_letters(word))\n",
    "                features[index - 1][2 * num_of_features + 5] = int(tokens[i - 1] == '\"')\n",
    "                features[index - 1][2 * num_of_features + 6] = int(tokens[i - 1] == '(')\n",
    "                features[index - 1][2 * num_of_features + 7] = int(tokens[i - 1] == ' ')\n",
    "                features[index - 1][2*num_of_features + 8] = int(tokens[i - 1] == ')')\n",
    "                features[index - 1][2*num_of_features + 9] = int(tokens[i - 1] == '-')\n",
    "                if( i + 1 < len(tokens)):\n",
    "                    features[index - 1][2 * num_of_features + 10] = int(tokens[i + 1] == ')')\n",
    "                    features[index - 1][2 * num_of_features + 11] = int(tokens[i + 1] == '\"')\n",
    "                    features[index - 1][2 * num_of_features + 12] = int(tokens[i + 1] == ' ')\n",
    "                    features[index - 1][2*num_of_features + 13] = int(tokens[i + 1] == ')')\n",
    "                    features[index - 1][2*num_of_features + 14] = int(tokens[i + 1] == '-')\n",
    "                \n",
    "                for j, char in enumerate(list(word)):\n",
    "                    features[index - 1][2 * num_of_features + 15 + j] = encoding[char.upper()]\n",
    "                \n",
    "                features[index][:2 * num_of_features] = features[index - 1][num_of_features:]\n",
    "            \n",
    "            # Присваивание лейбла\n",
    "            \n",
    "            if(word[0].isupper()):\n",
    "                if(word.isupper()):\n",
    "                    labels[index] = 2\n",
    "                else:\n",
    "                    labels[index] = 1\n",
    "            else:\n",
    "                labels[index] = 0\n",
    "            index += 1\n",
    "            \n",
    "    if(not test):    \n",
    "        return list(features), list(labels)\n",
    "    else:\n",
    "        return list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter({'О': 35312904, ' ': 25422815, 'Н': 24537989, 'Е': 23969940, 'Т': 21420192, 'С': 19892386, 'А': 16809787, 'И': 14950389, 'Р': 13874547, 'В': 13280775, '\"': 9695019, 'К': 7858626, 'Г': 5914467, 'Л': 5851872, 'Б': 5428314, 'Й': 5352151, '\\n': 5232476, 'Ь': 4866345, 'П': 4642709, 'Д': 4606840, 'М': 4369257, 'Ч': 3898716, 'Я': 3613597, 'У': 3592997, 'Щ': 3302510, 'Ю': 3018424, 'З': 2625035, 'Ц': 2040518, 'Ы': 1500303, '-': 1443125, 'Ж': 1273203, 'Ф': 1136416, 'Х': 1122008, 'Ш': 674331, 'Э': 502713, '.': 406987, '№': 214440, '1': 194707, '2': 167236, '(': 124233, ')': 124117, '0': 102588, ',': 95619, '3': 92674, '4': 73813, '5': 68067, '7': 58772, '9': 58326, '6': 56049, 'Ъ': 55448, '8': 47595, 'Ё': 29626, 'I': 29581, '/': 28983, '+': 25175, 'X': 13675, \"'\": 12332, '`': 8071, 'N': 7230, 'T': 5980, 'V': 5571, 'O': 5074, 'A': 5020, 'E': 4842, 'R': 4648, 'L': 4577, 'S': 4524, 'C': 4156, 'P': 3837, 'D': 3460, 'M': 2589, ':': 1590, '!': 1578, '&': 1530, 'U': 1491, 'G': 1320, '<': 1152, 'H': 1102, '>': 1069, 'F': 1058, 'Y': 1054, 'B': 1005, 'K': 927, ';': 892, '\\\\': 634, 'W': 541, '°': 509, '«': 427, 'Z': 379, '»': 378, '*': 374, '%': 237, '_': 237, 'J': 226, '?': 120, 'Q': 58, '=': 48, '@': 45, '–': 41, '^': 29, '{': 24, '}': 23, '[': 16, '~': 16, ']': 14, '$': 12, '#': 12, '‹': 10, '›': 10, '·': 8, '|': 7, '“': 6, '”': 6, '©': 3, '—': 2, '§': 2, '’': 2, '™': 2, '®': 1, 'Є': 1, '\\xa0': 1, '±': 1, '…': 1, '¶': 1})\n",
    "print('Число уникальных символов :', len(counter))\n",
    "encoding = {}\n",
    "i = 0\n",
    "for char in counter:\n",
    "    encoding[char] = i/len(counter)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(companies, test=False):\n",
    "    features = np.array([])\n",
    "    labels = np.array([])\n",
    "    for company in companies:\n",
    "        curr_features, curr_labels = extract_features(company)\n",
    "        if(len(features)):\n",
    "            features += curr_features\n",
    "            labels += curr_labels\n",
    "        else:\n",
    "            features = curr_features\n",
    "            labels = curr_labels\n",
    "            \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(company, labels):\n",
    "    word_index = 0\n",
    "    corrected = tokenize(company, spaces=True)\n",
    "    for i, word in enumerate(corrected):\n",
    "        if (has_letters(word)):\n",
    "            if (labels[word_index] == 0):\n",
    "                corrected[i] = word.lower()\n",
    "            elif(labels[word_index] == 1):\n",
    "                corrected[i] = word[0] + word[1:].lower()\n",
    "            word_index += 1\n",
    "    return ''.join(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Создаю кастомный датасет</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedWordsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        \n",
    "        mixed_words = []\n",
    "        for company in data:\n",
    "            for word in tokenize(company):\n",
    "                if not word[1:].islower() and not word[1:].isupper() and len(word) > 2:\n",
    "                    mixed_words.append(word)\n",
    "        \n",
    "        features = np.zeros((len(mixed_words), MAX_WORD))\n",
    "        labels = np.zeros((len(mixed_words), MAX_WORD))\n",
    "        for i in range(len(mixed_words)):\n",
    "            for j in range(len(mixed_words[i])):\n",
    "                features[i][j] = encoding[mixed_words[i][j].upper()]\n",
    "                labels[i][j] = int(mixed_words[i][j].isupper())\n",
    "        self.X = torch.tensor(features)\n",
    "        self.y = torch.tensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MixedWordsDataset(train_companies[:100000])\n",
    "val_dataset = MixedWordsDataset(train_companies[100000:120000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Цикл обучения</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                dataloader = train_dataloader\n",
    "                model.train()  \n",
    "            else:\n",
    "                dataloader = val_dataloader\n",
    "                model.eval()   \n",
    "\n",
    "            running_loss = 0.\n",
    ".\n",
    "            for inputs, labels in tqdm(dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    preds = model(inputs.float())\n",
    "                    loss_value = loss(preds, labels.float())\n",
    "                    preds_class = preds.argmax(dim=1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss_value.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss_value.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss), flush=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Простейшая нейросеть с полносвязными слоями</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class WordNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_features):\n",
    "        \n",
    "        super(WordNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_of_features, 200) \n",
    "        self.fc2 = nn.Linear(200, 120)\n",
    "        self.fc3 = nn.Linear(120, 84)\n",
    "        self.fc4 = nn.Linear(84, MAX_WORD)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordNet(MAX_WORD)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3*1.0e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:05<00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5512\n",
      "Epoch 1/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 2/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 3/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 4/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 5/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:04<00:00,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 6/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 7/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 8/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 9/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 10/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 11/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 12/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 13/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 14/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 15/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 16/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 17/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 18/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n",
      "Epoch 19/19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 29/29 [00:03<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7/7 [00:03<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WordNet(\n",
       "  (fc1): Linear(in_features=57, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=120, bias=True)\n",
       "  (fc3): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc4): Linear(in_features=84, out_features=57, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, loss, optimizer, scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">За 20 эпох лосс так и не начал нормально уменьшаться</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
